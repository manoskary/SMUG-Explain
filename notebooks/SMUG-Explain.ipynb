{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNY1dIySaoaidFCdGBZLuzo"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Welcome to the SMUG-Explain Inference for Cadences Colab\n",
    "\n",
    "In this notebook you will learn how to follow the installation instructions and the inference process to generate explanation subgraphs and cadence predictions.\n",
    "\n",
    "First steps would be to download the repo and install the dependencies."
   ],
   "metadata": {
    "id": "98N9JY1G5Xh9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwM1uyTx5QBE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1709902621003,
     "user_tz": -60,
     "elapsed": 1306,
     "user": {
      "displayName": "Manos Karistineos",
      "userId": "12441812386719824383"
     }
    },
    "outputId": "530100d2-8260-46d0-cfa8-e1222ef8d4fd"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/manoskary/SMUG-Explain.git"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pyg-nightly\n",
    "# !pip install --verbose git+https://github.com/pyg-team/pyg-lib.git\n",
    "!pip install --verbose torch_scatter\n",
    "# !pip install --verbose torch_sparse\n",
    "# !pip install --verbose torch_cluster\n",
    "# !pip install --verbose torch_spline_conv\n",
    "!pip install pytorch_lightning partitura captum"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGgPFtVg51oC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1709905234708,
     "user_tz": -60,
     "elapsed": 1836377,
     "user": {
      "displayName": "Manos Karistineos",
      "userId": "12441812386719824383"
     }
    },
    "outputId": "789c78c1-d382-42d0-ff34-de320bd3facf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing the necessary dependencies. Be patient, this may take a while."
   ],
   "metadata": {
    "id": "6hDwEjiB6ots"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import partitura as pt\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.explain import CaptumExplainer, Explainer, GNNExplainer, GraphMaskExplainer, fidelity, characterization_score\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import sys"
   ],
   "metadata": {
    "id": "tdqeRGTV5701",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1709905257026,
     "user_tz": -60,
     "elapsed": 7585,
     "user": {
      "displayName": "Manos Karistineos",
      "userId": "12441812386719824383"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import local imports from the SMUG-Explain repo."
   ],
   "metadata": {
    "id": "rv46NRa3QkWD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), \"SMUG-Explain\", \"python\"))\n",
    "from model import CadencePLModel\n",
    "from utils import CadenceEncoder, save_pyg_graph_as_json, hetero_fidelity, create_score_graph\n",
    "from features import cadence_features"
   ],
   "metadata": {
    "id": "aXq5E9CQQjdB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1709905262598,
     "user_tz": -60,
     "elapsed": 2919,
     "user": {
      "displayName": "Manos Karistineos",
      "userId": "12441812386719824383"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next block we will define the explain function."
   ],
   "metadata": {
    "id": "eogl0DB_6b8N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def explain(model, batch, feature_labels=None, explanation_type=\"model\", algorithm=CaptumExplainer('IntegratedGradients')):\n",
    "    x_dict = batch.x_dict\n",
    "    labels = batch[\"note\"].y if explanation_type == \"phenomenon\" else None\n",
    "    edge_index_dict = batch.edge_index_dict\n",
    "    pytorch_model = model.module\n",
    "    pytorch_model.eval()\n",
    "    explainer = Explainer(\n",
    "        model=pytorch_model,\n",
    "        algorithm=algorithm,\n",
    "        explanation_type=explanation_type,\n",
    "        model_config=dict(\n",
    "            mode='multiclass_classification',\n",
    "            task_level='node',\n",
    "            return_type='probs',\n",
    "        ),\n",
    "        node_mask_type='attributes',\n",
    "        edge_mask_type='object',\n",
    "        threshold_config = {\"threshold_type\": 'topk_hard', \"value\": 10}\n",
    "    )\n",
    "\n",
    "    # Get the edge mask for each note\n",
    "    save_path = os.path.join(os.getcwd(), \"artifacts\", \"explanations\", batch.name)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    edge_mask = dict()\n",
    "    preds = pytorch_model(x_dict, edge_index_dict).argmax(dim=-1)\n",
    "    pos_fidelities = []\n",
    "    neg_fidelities = []\n",
    "    for note_idx in tqdm.tqdm(range(x_dict[\"note\"].size(0)), desc=\"Explaining notes ... \"):\n",
    "        note = dict()\n",
    "        # Only give explanations when preds != 0 (no cadence) or labels != 0\n",
    "        if labels is not None:\n",
    "            if preds[note_idx] == 0 and labels[note_idx] == 0:\n",
    "                continue\n",
    "        else:\n",
    "            if preds[note_idx] == 0:\n",
    "                continue\n",
    "\n",
    "        explanation = explainer(x_dict, edge_index_dict, index=note_idx, target=labels if labels is not None else None)\n",
    "        fidelity_score = hetero_fidelity(explainer, explanation)\n",
    "        pos_fidelities.append(fidelity_score[0])\n",
    "        neg_fidelities.append(fidelity_score[1])\n",
    "        for k in [\"onset\", \"consecutive\", \"during\", \"rest\"]:\n",
    "            eem = explanation[\"note\", k, \"note\"].edge_mask\n",
    "            edge_index = batch[\"note\", k, \"note\"].edge_index\n",
    "            note[k] = edge_index[:, eem > 0].tolist()\n",
    "        xml_idx = batch[\"note\"].id[note_idx]\n",
    "        featimp_dict = dict()\n",
    "        feature_importance = explanation[\"note\"].node_mask.sum(dim=0)\n",
    "        for i, f_name in enumerate(feature_labels):\n",
    "            featimp_dict[f_name] = feature_importance[i].item()\n",
    "        note[\"feature_importance\"] = featimp_dict\n",
    "        edge_mask[xml_idx] = note\n",
    "    print(\"Mean Positive Fidelity:\", np.mean(pos_fidelities))\n",
    "    print(\"Mean Negative Fidelity:\", np.mean(neg_fidelities))\n",
    "    char_score = characterization_score(torch.tensor(pos_fidelities), torch.tensor(neg_fidelities))\n",
    "    print(\"Characterization Score:\", char_score.mean())\n",
    "    torch.save(char_score, os.path.join(save_path, \"characterization_score.pt\"))\n",
    "    return edge_mask"
   ],
   "metadata": {
    "id": "o3ATCdcl6cNT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1709905318329,
     "user_tz": -60,
     "elapsed": 732,
     "user": {
      "displayName": "Manos Karistineos",
      "userId": "12441812386719824383"
     }
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "19x3AMRv6zkk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def main(test_score):\n",
    "    artifact_dir = os.path.join(os.getcwd(), \"SMUG-Explain\", \"assets\")\n",
    "    # load model from checkpoint\n",
    "    model = CadencePLModel.load_from_checkpoint(os.path.join(artifact_dir,  \"model.ckpt\"))\n",
    "    # compile for faster inference\n",
    "    torch.compile(model, dynamic=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Get graph from score\n",
    "    if test_score is None or not os.path.exists(test_score):\n",
    "        raise ValueError(\"No score found or invalid path. Please provide a valid score to test.\")\n",
    "    else:\n",
    "        score_name = os.path.basename(os.path.normpath(test_score))\n",
    "        score = pt.load_score(os.path.normpath(test_score))\n",
    "\n",
    "        model.module.eval()\n",
    "        cadence_encoder = CadenceEncoder()\n",
    "        part = score.parts[-1] if isinstance(score, pt.score.Score) else score\n",
    "        # Remove Grace notes\n",
    "        grace_notes = list(part.iter_all(pt.score.GraceNote))\n",
    "        for grace in grace_notes:\n",
    "            part.remove(grace)\n",
    "        # Remove Roman numerals\n",
    "        note_array = part.note_array(include_time_signature=True, include_metrical_position=True,\n",
    "                                     include_pitch_spelling=True)\n",
    "        # Remove previous cadences and Roman numerals\n",
    "        labels = cadence_encoder.encode(note_array, part.cadences)\n",
    "        # Only keep cadences [0, 1, 2, 3] i.e. NoCad, PAC, IAC, HC\n",
    "        labels[labels > 3] = 0\n",
    "        explanation_type = \"model\"\n",
    "        # Remove previous cadences\n",
    "        for cad in part.cadences:\n",
    "            part.remove(cad)\n",
    "        # Get the graph from the score\n",
    "        features, feature_labels = cadence_features(note_array)\n",
    "        graph = create_score_graph(features, note_array, labels=labels)\n",
    "        graph.name = os.path.splitext(score_name)[0]\n",
    "        graph[\"note\"].id = note_array[\"id\"]\n",
    "        graph[\"note\"].feature_labels = feature_labels\n",
    "        graph = graph.to(device=\"cpu\" if accelerator == \"cpu\" else devices[0])\n",
    "        pytorch_model = model.module\n",
    "        pytorch_model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = pytorch_model(graph.x_dict, graph.edge_index_dict).argmax(dim=-1)\n",
    "        cadence_decoder = cadence_encoder.decode(predictions)\n",
    "        cad_ids = np.where(cadence_decoder != \"\")\n",
    "        prev_onset = -1\n",
    "        for idx in cad_ids[0]:\n",
    "            cad_type = cadence_decoder[idx]\n",
    "            onset_div = note_array[\"onset_div\"][idx]\n",
    "            if onset_div == prev_onset:\n",
    "                continue\n",
    "            if cadence_decoder[idx-1] != \"\":\n",
    "                print(f\"Conflicting Cadence {cad_type} at {onset_div}!\")\n",
    "            part.add(pt.score.Cadence(cad_type), onset_div)\n",
    "            prev_onset = onset_div\n",
    "        pt.score.infer_beaming(part)\n",
    "        pt.save_mei(score, os.getcwd(), \"artifacts\", \"explanations\", f\"{graph.name}_explained.mei\", title=os.path.splitext(score_name)[0])\n",
    "        # Get the explanation algorithm name by default it is Integrated Gradients\n",
    "        graph.name = graph.name + \"_\" + explanation_type + \"_IG\"\n",
    "\n",
    "\n",
    "        # Get explanations for a score\n",
    "        edge_mask = explain(model, graph, feature_labels)\n",
    "        save_pyg_graph_as_json(graph, note_array[\"id\"], extra_info=edge_mask, path=os.path.join(os.getcwd(), \"artifacts\", \"explanations\"))\n",
    "\n"
   ],
   "metadata": {
    "id": "5fcJwLh46z9X",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1709905360029,
     "user_tz": -60,
     "elapsed": 286,
     "user": {
      "displayName": "Manos Karistineos",
      "userId": "12441812386719824383"
     }
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Uploading test files.\n",
    "\n",
    "Ok, you are now done with the installation phase and you are ready to run inference on scores. To do this step you need first to upload your scores in one of the following formats:\n",
    "- MEI\n",
    "- MusicXML\n",
    "- MIDI\n",
    "- MuseScore\n",
    "- Kern\n",
    "\n",
    "But it needs to contain a single part to export a readable representation on the SMUG-Explain web interface.\n",
    "\n",
    "To do this step you can navigate to the sidebar on the left and upload a score.\n",
    "Keep in mind the path on which you saved the uploaded score because you will need it later, by default the path should be: `os.path.join(os.getcwd())`"
   ],
   "metadata": {
    "id": "WjACNHcxJfF8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "my_score_path = \"Path/to/your/score\"\n",
    "# default_path = os.path.join(os.getcwd(), \"my_score.musicxml\")\n",
    "\n",
    "main(my_score_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "uKEFZVgyI5Fs",
    "executionInfo": {
     "status": "error",
     "timestamp": 1709905367145,
     "user_tz": -60,
     "elapsed": 1629,
     "user": {
      "displayName": "Manos Karistineos",
      "userId": "12441812386719824383"
     }
    },
    "outputId": "e6117176-8687-47c5-899b-41ab6f8082ed"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Almost Done!\n",
    "\n",
    "Once the code has finished running, you need to locate the generated explanations. These should be the generated JSON file and the generated MEI score.\n",
    "\n",
    "By default the path of these files should be in a folder `artifacts/explanations` in the current working directory.\n",
    "\n",
    "You can now navigate to the SMUG-Explain web interface and upload the generated JSON file to visualize the explanation subgraphs and the MEI file to visualize the annotated score.\n",
    "\n",
    "Happy Explaining!"
   ],
   "metadata": {
    "id": "h_l0Jp9bKpKE"
   }
  }
 ]
}
